/**
 * ì‹¤ì‹œê°„ ì–¼êµ´ ì¶”ì  ì»´í¬ë„ŒíŠ¸
 * - ì›¹ìº ì„ í†µí•œ ì‹¤ì‹œê°„ ì–¼êµ´ ì¸ì‹
 * - í‘œì •, ë‚˜ì´, ì„±ë³„ ì‹¤ì‹œê°„ í‘œì‹œ
 * - ì–¼êµ´ ëœë“œë§ˆí¬ ë° bounding box ì‹œê°í™”
 * - í˜„ì¬ ì–¼êµ´ë¡œ ê¸°ì¤€ ì‚¬ì§„ ë“±ë¡ ê¸°ëŠ¥
 */

'use client';

import { useEffect, useRef, useState } from 'react';
import * as faceapi from 'face-api.js';
import { getDominantExpression, getExpressionEmoji, getExpressionLabel } from '@/lib/faceRecognition';

interface LiveFaceTrackerProps {
  isModelsLoaded: boolean;
  onCaptureFace: (file: File) => void;
}

export default function LiveFaceTracker({
  isModelsLoaded,
  onCaptureFace,
}: LiveFaceTrackerProps) {
  const videoRef = useRef<HTMLVideoElement>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const animationFrameRef = useRef<number | null>(null);
  
  const [isStreaming, setIsStreaming] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [detectionInfo, setDetectionInfo] = useState<{
    faceCount: number;
    expression?: string;
    expressionEmoji?: string;
    expressionProb?: number;
    age?: number;
    gender?: string;
  } | null>(null);

  // ì›¹ìº  ì‹œì‘
  const startWebcam = async () => {
    try {
      setError(null);
      const stream = await navigator.mediaDevices.getUserMedia({
        video: {
          width: { ideal: 1280 },
          height: { ideal: 720 },
          facingMode: 'user',
        },
      });

      if (videoRef.current) {
        videoRef.current.srcObject = stream;
        streamRef.current = stream;
        setIsStreaming(true);
      }
    } catch (err) {
      console.error('ì›¹ìº  ì ‘ê·¼ ì‹¤íŒ¨:', err);
      setError('ì›¹ìº ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¹´ë©”ë¼ ê¶Œí•œì„ í™•ì¸í•´ ì£¼ì„¸ìš”.');
    }
  };

  // ì›¹ìº  ì¤‘ì§€
  const stopWebcam = () => {
    if (streamRef.current) {
      streamRef.current.getTracks().forEach((track) => track.stop());
      streamRef.current = null;
    }
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
    }
    setIsStreaming(false);
    setDetectionInfo(null);
  };

  // ì‹¤ì‹œê°„ ì–¼êµ´ ê²€ì¶œ ë£¨í”„
  const detectFaces = async () => {
    if (!videoRef.current || !canvasRef.current || !isModelsLoaded || !isStreaming) {
      return;
    }

    const video = videoRef.current;
    const canvas = canvasRef.current;

    // ë¹„ë””ì˜¤ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ëŒ€ê¸°
    if (video.readyState !== 4) {
      animationFrameRef.current = requestAnimationFrame(detectFaces);
      return;
    }

    const displaySize = {
      width: video.videoWidth,
      height: video.videoHeight,
    };

    // Canvas í¬ê¸° ì„¤ì •
    if (canvas.width !== displaySize.width || canvas.height !== displaySize.height) {
      faceapi.matchDimensions(canvas, displaySize);
    }

    try {
      // ì–¼êµ´ ê²€ì¶œ (ëª¨ë“  ì •ë³´ í¬í•¨)
      const detections = await faceapi
        .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions({
          inputSize: 224,
          scoreThreshold: 0.5,
        }))
        .withFaceLandmarks()
        .withFaceExpressions()
        .withAgeAndGender();

      const resizedDetections = faceapi.resizeResults(detections, displaySize);

      // Canvas ì´ˆê¸°í™”
      const ctx = canvas.getContext('2d');
      if (ctx) {
        ctx.clearRect(0, 0, canvas.width, canvas.height);
      }

      // ê²€ì¶œëœ ì–¼êµ´ ê·¸ë¦¬ê¸°
      if (resizedDetections.length > 0) {
        // Bounding boxì™€ ëœë“œë§ˆí¬ ê·¸ë¦¬ê¸°
        faceapi.draw.drawDetections(canvas, resizedDetections);
        faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);

        // ì²« ë²ˆì§¸ ì–¼êµ´ ì •ë³´ ì €ì¥
        const detection = resizedDetections[0];
        const expressions = detection.expressions;
        const dominantExpr = Object.entries(expressions).reduce((a, b) =>
          a[1] > b[1] ? a : b
        );

        setDetectionInfo({
          faceCount: resizedDetections.length,
          expression: dominantExpr[0],
          expressionEmoji: getExpressionEmoji(dominantExpr[0] as any),
          expressionProb: dominantExpr[1],
          age: Math.round(detection.age),
          gender: detection.gender === 'male' ? 'ë‚¨ì„±' : 'ì—¬ì„±',
        });
      } else {
        setDetectionInfo({
          faceCount: 0,
        });
      }
    } catch (err) {
      console.error('ì–¼êµ´ ê²€ì¶œ ì˜¤ë¥˜:', err);
    }

    // ë‹¤ìŒ í”„ë ˆì„ (ì•½ 10fps)
    setTimeout(() => {
      animationFrameRef.current = requestAnimationFrame(detectFaces);
    }, 100);
  };

  // í˜„ì¬ í”„ë ˆì„ ìº¡ì²˜í•˜ì—¬ ê¸°ì¤€ ì‚¬ì§„ìœ¼ë¡œ ë“±ë¡
  const captureCurrentFace = () => {
    if (!videoRef.current || !canvasRef.current) return;

    const video = videoRef.current;
    const canvas = document.createElement('canvas');
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;

    const ctx = canvas.getContext('2d');
    if (ctx) {
      ctx.drawImage(video, 0, 0);
      canvas.toBlob((blob) => {
        if (blob) {
          const file = new File([blob], 'webcam-capture.jpg', { type: 'image/jpeg' });
          onCaptureFace(file);
        }
      }, 'image/jpeg', 0.95);
    }
  };

  // ë¹„ë””ì˜¤ ë¡œë“œ ì™„ë£Œ ì‹œ ê²€ì¶œ ì‹œì‘
  useEffect(() => {
    if (isStreaming && isModelsLoaded && videoRef.current) {
      videoRef.current.addEventListener('loadeddata', () => {
        detectFaces();
      });
    }

    return () => {
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
    };
  }, [isStreaming, isModelsLoaded]);

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì›¹ìº  ì¤‘ì§€
  useEffect(() => {
    return () => {
      stopWebcam();
    };
  }, []);

  return (
    <div className="space-y-4">
      <div className="flex justify-between items-center">
        <h2 className="text-xl font-semibold text-black">ì‹¤ì‹œê°„ ì–¼êµ´ ì¶”ì </h2>
        <div className="space-x-2">
          {!isStreaming ? (
            <button
              onClick={startWebcam}
              disabled={!isModelsLoaded}
              className={`px-4 py-2 rounded-lg font-semibold text-white transition-colors ${
                isModelsLoaded
                  ? 'bg-blue-600 hover:bg-blue-700'
                  : 'bg-gray-400 cursor-not-allowed'
              }`}
            >
              ì›¹ìº  ì‹œì‘
            </button>
          ) : (
            <>
              <button
                onClick={captureCurrentFace}
                className="px-4 py-2 bg-green-600 text-white rounded-lg hover:bg-green-700 transition-colors font-semibold"
              >
                í˜„ì¬ ì–¼êµ´ë¡œ ë“±ë¡
              </button>
              <button
                onClick={stopWebcam}
                className="px-4 py-2 bg-red-600 text-white rounded-lg hover:bg-red-700 transition-colors font-semibold"
              >
                ì›¹ìº  ì¤‘ì§€
              </button>
            </>
          )}
        </div>
      </div>

      {error && (
        <div className="bg-red-50 border border-red-200 rounded-lg p-4">
          <p className="text-red-700">{error}</p>
        </div>
      )}

      {/* ë¹„ë””ì˜¤ ë° Canvas */}
      <div className="relative inline-block bg-black rounded-lg overflow-hidden">
        <video
          ref={videoRef}
          autoPlay
          playsInline
          muted
          className="max-w-full h-auto"
          style={{ display: isStreaming ? 'block' : 'none' }}
        />
        <canvas
          ref={canvasRef}
          className="absolute top-0 left-0"
          style={{ display: isStreaming ? 'block' : 'none' }}
        />
        {!isStreaming && (
          <div className="w-full h-64 flex items-center justify-center bg-gray-200">
            <p className="text-gray-500">ì›¹ìº ì„ ì‹œì‘í•˜ë ¤ë©´ ìœ„ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”</p>
          </div>
        )}
      </div>

      {/* ê²€ì¶œ ì •ë³´ í‘œì‹œ */}
      {isStreaming && detectionInfo && (
        <div className="bg-white border border-gray-200 rounded-lg p-4">
          <h3 className="font-semibold text-lg mb-2 text-black">ê²€ì¶œ ì •ë³´</h3>
          {detectionInfo.faceCount > 0 ? (
            <div className="space-y-2">
              <p className="text-gray-700">
                ê²€ì¶œëœ ì–¼êµ´: <span className="font-semibold">{detectionInfo.faceCount}ëª…</span>
              </p>
              {detectionInfo.expression && (
                <p className="text-gray-700">
                  í‘œì •: {detectionInfo.expressionEmoji}{' '}
                  <span className="font-semibold">
                    {getExpressionLabel(detectionInfo.expression as any)}
                  </span>{' '}
                  ({Math.round((detectionInfo.expressionProb || 0) * 100)}%)
                </p>
              )}
              {detectionInfo.age && (
                <p className="text-gray-700">
                  ë‚˜ì´: <span className="font-semibold">ì•½ {detectionInfo.age}ì„¸</span>
                </p>
              )}
              {detectionInfo.gender && (
                <p className="text-gray-700">
                  ì„±ë³„: <span className="font-semibold">{detectionInfo.gender}</span>
                </p>
              )}
            </div>
          ) : (
            <p className="text-gray-500">ì–¼êµ´ì´ ê²€ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤</p>
          )}
        </div>
      )}

      <div className="bg-blue-50 border border-blue-200 rounded-lg p-4">
        <p className="text-blue-800 text-sm">
          ğŸ’¡ íŒ: "í˜„ì¬ ì–¼êµ´ë¡œ ë“±ë¡" ë²„íŠ¼ì„ í´ë¦­í•˜ë©´ í˜„ì¬ í”„ë ˆì„ì„ ê¸°ì¤€ ì–¼êµ´ ì‚¬ì§„ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        </p>
      </div>
    </div>
  );
}

